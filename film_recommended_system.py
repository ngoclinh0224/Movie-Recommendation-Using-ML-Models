# -*- coding: utf-8 -*-
"""Film_Recommended_System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NiAMa54OVeuVKoHrPlWy7rMX_HzUHkPi
"""

# importing required Libraries
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

#đọc file
df = pd.read_csv('sample_data/netflix_data.csv')
df.columns

df.head()

#Data Preprocessing and EDA
df.info()

df.isnull().sum()

df.fillna('', inplace=True)

df.describe(include='all').T

"""## EDA cơ bản"""

import plotly.graph_objects as go
movie_counts = df['release_year'].value_counts().sort_index()
fig = go.Figure(data=go.Bar(x=movie_counts.index, y=movie_counts.values))
fig.update_layout(
    plot_bgcolor='rgb(17, 17, 17)',
    paper_bgcolor='rgb(17, 17, 17)',
    font_color='white',
    title='Number of Movies Released Each Year',
    xaxis=dict(title='Year'),
    yaxis=dict(title='Number of Movies')
)
fig.update_traces(marker_color='red')
fig.show()

movie_type_counts = df['type'].value_counts()

fig = go.Figure(data=go.Pie(labels=movie_type_counts.index, values=movie_type_counts.values))

fig.update_layout(
    plot_bgcolor='rgb(17, 17, 17)',
    paper_bgcolor='rgb(17, 17, 17)',
    font_color='white',
    title='Distribution of C. Types',
)
fig.update_traces(marker=dict(colors=['red']))
fig.show()

import plotly.express as px
top_countries = df['country'].value_counts().head(10)

fig = px.treemap(names=top_countries.index, parents=["" for _ in top_countries.index], values=top_countries.values)

fig.update_layout(
    plot_bgcolor='rgb(17, 17, 17)',
    paper_bgcolor='rgb(17, 17, 17)',
    font_color='white',
    title='Top Countries with Highest Number of Movies',
)
fig.show()

country_movie_counts = df['country'].value_counts()

data = pd.DataFrame({'Country': country_movie_counts.index, 'Movie Count': country_movie_counts.values})

fig = px.choropleth(data_frame=data, locations='Country', locationmode='country names',
                    color='Movie Count', title='Number of Movies Released By Country',
                    color_continuous_scale='Reds', range_color=(0, data['Movie Count'].max()),
                    labels={'Movie Count': 'Number of Movies'})

fig.update_layout(
    plot_bgcolor='rgb(17, 17, 17)',
    paper_bgcolor='rgb(17, 17, 17)',
    font_color='white'
)
fig.show()

ratings       = list(df['duration'].value_counts().index)
rating_counts = list(df['duration'].value_counts().values)

fig = go.Figure(data=[go.Bar(
    x=ratings,
    y=rating_counts,
    marker_color='#E50914'
)])

fig.update_layout(
    title='Movie Durations Distribution',
    xaxis_title='Rating',
    yaxis_title='Count',
    plot_bgcolor='rgba(0, 0, 0, 0)',
    paper_bgcolor='rgba(0, 0, 0, 0.7)',
    font=dict(
        color='white'
    )
)

fig.show()

"""## Feature Enginering"""

new_data = df[['title', 'type', 'director', 'cast', 'rating', 'listed_in', 'description']]
new_data.set_index('title', inplace=True)

new_data.head()

"""## Random Forest

Dựa vào rating  để phân loại phim theo độ tuổi, từ đó có thể gợi ý phim cho người xem.
"""

from sklearn.preprocessing import LabelEncoder
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer

#Load the dataset
df = pd.read_csv('sample_data/netflix_data.csv')

#Encode duration thành 2 cột season và min sau đó add vào dataframe
# Tạo cột 'season'
df['season'] = df['duration'].apply(lambda x: int(x.split()[0]) if 'Season' in str(x) else 0)

# Tạo cột 'min'
df['min'] = df['duration'].apply(lambda x: int(x.split()[0]) if 'min' in str(x) else 0)

#Encode type
encoder = LabelEncoder()
df['type'] = encoder.fit_transform(df['type']) # movie = 0, tv show = 1

#Encode ratings
df['rating'] = encoder.fit_transform(df['rating'].astype(str))

#Encode country
df['country'] = encoder.fit_transform(df['country'].astype(str))

# Select relevant features
features = ['type', 'release_year', 'listed_in',
            'description', 'country', 'season', 'min', 'title']

#Splitting data into training and test sets
X = df[features]
y = df['rating']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a ColumnTransformer to handle categorical features
categorical_features = ['listed_in', 'description', 'title']
preprocessor = ColumnTransformer(
    transformers=[
        ('num', 'passthrough', ['type', 'release_year', 'country', 'season', 'min']),
        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)
    ])

# Preprocess the data
X_train = preprocessor.fit_transform(X_train)
X_test = preprocessor.transform(X_test)

#Train Random Forest model
rf = RandomForestClassifier(n_estimators=100, max_depth=50, random_state=42)
rf.fit(X_train, y_train)

"""Calculate Accuracy, Recall, Precision and F1 Score of RandomForest"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Accuracy
acc = accuracy_score(y_test, y_pred)

# Precision, Recall, F1 (macro: tính trung bình cho tất cả các lớp)
prec = precision_score(y_test, y_pred, average='macro', zero_division=0)
rec = recall_score(y_test, y_pred, average='macro', zero_division=0)
f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)

# In kết quả
print("Accuracy:", acc)
print("Precision:", prec)
print("Recall:", rec)
print("F1 Score:", f1)

"""As above, we already had some index value:
- Accuracy = 0.3757 = 37.57%
- Precision = 0.0313 = 3.13%
- Recall = 0.8333 = 83.33%
- F1 Score = 0.0455 = 4.55%

The value of these indes is low, which means that the prediction of the model is not good. We have check the distribution of the data again and decided to aplly GridSearch to gain better result.
"""

import pandas as pd
pd.Series(y_train).value_counts()

"""Applying GridSearchCV"""

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [10, 20, None],
    'class_weight': ['balanced']
}

grid = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, scoring='f1_macro', cv=3)
grid.fit(X_train, y_train)
print(grid.best_params_)

"""Calculate Accuracy, Precision, Recall and F1 Score again"""

# Accuracy
acc = accuracy_score(y_test, y_pred)

# Precision, Recall, F1 (macro: tính trung bình cho tất cả các lớp)
prec = precision_score(y_test, y_pred, average='macro', zero_division=0)
rec = recall_score(y_test, y_pred, average='macro', zero_division=0)
f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)

# In kết quả
print("Accuracy:", acc)
print("Precision:", prec)
print("Recall:", rec)
print("F1 Score:", f1)

"""We got:
- Accuracy = 0.5482 = 54.82%
- Precision = 0.3720 = 37.20%
- Recall = 0.3242 = 32.42%
- F1 Score = 0.3303 = 33.03%

As we see above, the accuracy increases from 37.57% to 54.82%, a difference of 17.25%.

The precision raises from 3.13% to 37.20%, a big gapwhen recall decreases to 34.42% from 83.33%.

F1 Score from 4.55% to 33.03%, a gap of 28.48%.

Conclusion:

These results highlight the importance of proper hyperparameter selection, especially in multi-class and imbalanced classification problems.

The use of macro averaging was appropriate as it provides a fair evaluation across all classes, including those with fewer samples. Nonetheless, the relatively low scores suggest further improvements can be made by:

- Applying data balancing techniques such as SMOTE or ADASYN.

- Experimenting with alternative models like XGBoost or SVM.

- For textual data, exploring TF-IDF with cosine similarity.

## KNN
"""



"""## XGBoost"""



"""## TF-IDF + Cosine"""

